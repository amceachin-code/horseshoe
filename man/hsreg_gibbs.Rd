% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hsreg_gibbs.R
\name{hsreg_gibbs}
\alias{hsreg_gibbs}
\title{Horseshoe Gibbs Sampler with Selective Penalization}
\usage{
hsreg_gibbs(
  y,
  X,
  penalized = NULL,
  lambda_scale = 1,
  tau_scale = 1,
  n_mcmc = 1000,
  burnin = 500,
  thin = 1,
  verbose = TRUE
)
}
\arguments{
\item{y}{Numeric vector of length n. The outcome variable.}

\item{X}{Numeric matrix of dimension n x p. The design matrix. Include an
intercept column explicitly if you want an unpenalized intercept.}

\item{penalized}{Logical vector of length p, or \code{NULL}. \code{TRUE}
means the corresponding coefficient gets the horseshoe prior (shrinkage);
\code{FALSE} means a flat prior (no shrinkage). Default (\code{NULL}):
all coefficients are penalized (standard horseshoe).}

\item{lambda_scale}{Positive scalar. Scale of the half-Cauchy prior on the
local shrinkage parameters: \eqn{\lambda_j \sim C^+(0, \text{lambda\_scale})}.
Larger values allow individual coefficients to escape shrinkage more
easily. Default: 1.}

\item{tau_scale}{Positive scalar. Scale of the half-Cauchy prior on the
global shrinkage parameter: \eqn{\tau \sim C^+(0, \text{tau\_scale})}.
Smaller values enforce more aggressive overall shrinkage. Default: 1.}

\item{n_mcmc}{Positive integer. Number of post-burnin MCMC draws to store.
Default: 1000.}

\item{burnin}{Non-negative integer. Number of initial iterations to discard.
Default: 500.}

\item{thin}{Positive integer. Keep every \code{thin}-th draw after burnin.
Default: 1 (no thinning).}

\item{verbose}{Logical. Print progress every 500 iterations. Default: TRUE.}
}
\value{
A named list with components:
  \describe{
    \item{beta_draws}{p x n_mcmc matrix of posterior draws for beta.}
    \item{sigma2_draws}{Numeric vector of length n_mcmc: posterior draws
      for \eqn{\sigma^2}.}
    \item{tau2_draws}{Numeric vector of length n_mcmc: posterior draws
      for \eqn{\tau^2}.}
    \item{lambda2_draws}{p_pen x n_mcmc matrix of posterior draws for
      \eqn{\lambda_j^2} (only for penalized coefficients, in the order
      they appear in X).}
    \item{pen_idx}{Integer vector: which columns of X are penalized.}
    \item{free_idx}{Integer vector: which columns of X are unpenalized.}
    \item{n_chol_fallback}{Integer: number of iterations where a ridge
      adjustment was needed for the Cholesky factorization.}
  }
}
\description{
Samples from the posterior of Bayesian linear regression with the horseshoe
prior applied only to a user-specified subset of coefficients. Unpenalized
coefficients receive a flat (improper) prior â€” equivalent to an OLS-like
posterior for those parameters, conditional on the penalized parameters.
}
\details{
This is the raw sampler that returns posterior draws directly. For a
higher-level interface with S3 methods (print, summary, predict, etc.),
see \code{\link{hsreg}}.


The sampler follows Makalic & Schmidt (2015) Algorithm 1, with the
modification that the horseshoe hierarchy (local shrinkage \eqn{\lambda_j},
global shrinkage \eqn{\tau}, and their auxiliary variables \eqn{\nu_j},
\eqn{\xi}) only governs the penalized subset of coefficients. Unpenalized
coefficients enter the posterior of \eqn{\beta} through the likelihood
(\eqn{X^T X}) term only, with no prior penalty.

The prior precision matrix \eqn{\Lambda^{*-1}} has:
\itemize{
  \item Entry \eqn{1/(\tau^2 \lambda_j^2)} at position (j,j) for penalized j
  \item Entry 0 at position (j,j) for unpenalized j (flat prior)
}

The \eqn{\sigma^2} and \eqn{\tau^2} conditionals sum only over penalized
coefficients, since unpenalized betas contribute no prior information to
these hyperparameters.

**Cholesky fallback**: If the Cholesky factorization of the precision
matrix \eqn{A = X^T X + \Lambda^{*-1}} fails (numerically singular), a
small ridge (\code{1e-8 * I_p}) is added and the factorization retried.
The count of such events is returned in \code{n_chol_fallback}. Persistent
fallbacks suggest near-collinear design columns.

**Divergence detection**: After each sigma2 and tau2 draw, the sampler
checks for NaN or extreme values (> 1e200). If detected, the sampler
stops with an informative error message identifying the iteration.

**Propriety requirement**: The posterior is proper when \eqn{n > p_{free}}
(number of unpenalized parameters) and the columns of X corresponding to
free parameters have full column rank.
}
\examples{
# Simple example: 5 true signals in 50 predictors
set.seed(123)
n <- 100; p <- 50
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(rep(2, 5), rep(0, 45))
y <- X \%*\% beta_true + rnorm(n)

fit <- hsreg_gibbs(y, X, n_mcmc = 200, burnin = 100, verbose = FALSE)
b_hat <- rowMeans(fit$beta_draws)
plot(beta_true, b_hat, xlab = "True", ylab = "Estimated")
abline(0, 1, col = "red")

}
\references{
Carvalho, C. M., Polson, N. G., & Scott, J. G. (2010). The horseshoe
estimator for sparse signals. \emph{Biometrika}, 97(2), 465-480.

Makalic, E. & Schmidt, D. F. (2015). A simple sampler for the horseshoe
estimator. \emph{arXiv:1508.03884v4}.
}
