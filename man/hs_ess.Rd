% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{hs_ess}
\alias{hs_ess}
\title{Effective Sample Size for an MCMC Chain}
\usage{
hs_ess(chain)
}
\arguments{
\item{chain}{Numeric vector. A single MCMC chain (posterior draws for one
parameter). Must have length >= 1.}
}
\value{
A scalar numeric value: the estimated effective sample size, floored
  at 1 (to avoid division-by-zero in downstream calculations). The ESS is
  always in the range \code{[1, length(chain)]}.
}
\description{
Computes the effective sample size (ESS) of a single MCMC chain using the
initial positive sequence estimator (Geyer, 1992). This is the same approach
used by Stan's \code{monitor()} and R's \code{coda::effectiveSize()}.
}
\details{
The ESS measures how many independent draws from the posterior an MCMC chain
of length \code{n} is equivalent to, accounting for autocorrelation between
successive draws. A chain with no autocorrelation has ESS = n; a highly
autocorrelated chain may have ESS << n.


The algorithm works as follows:
\enumerate{
  \item Center the chain by subtracting its mean.
  \item Compute the lag-0 variance (denominator for autocorrelation).
  \item Sum consecutive pairs of autocorrelations:
    \eqn{(\rho_{2k}, \rho_{2k+1})} for \eqn{k = 0, 1, 2, \ldots}
  \item Stop when the pair sum goes negative (initial positive sequence
    criterion from Geyer, 1992).
  \item Return \eqn{n / (-1 + 2 \cdot \text{rho\_sum})}.
}

For a constant chain (zero variance), ESS = n is returned, since every
draw is identical and there is no information loss from autocorrelation.
}
\examples{
# IID draws — ESS should be close to n
set.seed(42)
iid_chain <- rnorm(1000)
hs_ess(iid_chain)

# Highly autocorrelated chain — ESS << n
ar1_chain <- numeric(1000)
ar1_chain[1] <- rnorm(1)
for (i in 2:1000) ar1_chain[i] <- 0.99 * ar1_chain[i-1] + rnorm(1, sd = 0.1)
hs_ess(ar1_chain)

}
\references{
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. \emph{Statistical
Science}, 7(4), 473-483.
}
